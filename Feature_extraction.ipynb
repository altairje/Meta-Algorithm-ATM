{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tsfresh as tsf\n",
    "from tsfresh.feature_extraction import settings\n",
    "from pathlib import Path\n",
    "import pycatch22\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import linregress\n",
    "from scipy.fft import fft, fftfreq\n",
    "from statsmodels.tsa.stattools import acf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\local_global_res_13_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>train_time</th>\n",
       "      <th>forecast_time</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MASE</th>\n",
       "      <th>RMSSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>SMAPE</th>\n",
       "      <th>naming_orig</th>\n",
       "      <th>model_name</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>horizon</th>\n",
       "      <th>split</th>\n",
       "      <th>pred_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812914</td>\n",
       "      <td>4.442261</td>\n",
       "      <td>18.260277</td>\n",
       "      <td>619.323792</td>\n",
       "      <td>24.886217</td>\n",
       "      <td>0.896638</td>\n",
       "      <td>0.792087</td>\n",
       "      <td>inf</td>\n",
       "      <td>16.386864</td>\n",
       "      <td>danish_atm_daily_5</td>\n",
       "      <td>CatBoostAutoRegressivePipelineEtna_3lags_gl</td>\n",
       "      <td>danish_atm_daily</td>\n",
       "      <td>30</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.485261</td>\n",
       "      <td>4.376619</td>\n",
       "      <td>28.395468</td>\n",
       "      <td>1160.189331</td>\n",
       "      <td>34.061552</td>\n",
       "      <td>1.088141</td>\n",
       "      <td>0.918072</td>\n",
       "      <td>41.157335</td>\n",
       "      <td>17.157184</td>\n",
       "      <td>danish_atm_daily_84</td>\n",
       "      <td>CatBoostAutoRegressivePipelineEtna_3lags_gl</td>\n",
       "      <td>danish_atm_daily</td>\n",
       "      <td>30</td>\n",
       "      <td>validation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.812914</td>\n",
       "      <td>4.442261</td>\n",
       "      <td>12.825266</td>\n",
       "      <td>269.356567</td>\n",
       "      <td>16.412086</td>\n",
       "      <td>0.814535</td>\n",
       "      <td>0.757765</td>\n",
       "      <td>39.022604</td>\n",
       "      <td>13.038494</td>\n",
       "      <td>danish_atm_daily_32</td>\n",
       "      <td>CatBoostAutoRegressivePipelineEtna_3lags_gl</td>\n",
       "      <td>danish_atm_daily</td>\n",
       "      <td>30</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.485261</td>\n",
       "      <td>4.376619</td>\n",
       "      <td>26.284388</td>\n",
       "      <td>1121.348877</td>\n",
       "      <td>33.486548</td>\n",
       "      <td>0.918232</td>\n",
       "      <td>0.792925</td>\n",
       "      <td>57.078475</td>\n",
       "      <td>18.962322</td>\n",
       "      <td>danish_atm_daily_25</td>\n",
       "      <td>CatBoostAutoRegressivePipelineEtna_3lags_gl</td>\n",
       "      <td>danish_atm_daily</td>\n",
       "      <td>30</td>\n",
       "      <td>validation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.485261</td>\n",
       "      <td>4.376619</td>\n",
       "      <td>15.145726</td>\n",
       "      <td>429.357697</td>\n",
       "      <td>20.720948</td>\n",
       "      <td>0.663444</td>\n",
       "      <td>0.597604</td>\n",
       "      <td>21.996029</td>\n",
       "      <td>10.027625</td>\n",
       "      <td>danish_atm_daily_6</td>\n",
       "      <td>CatBoostAutoRegressivePipelineEtna_3lags_gl</td>\n",
       "      <td>danish_atm_daily</td>\n",
       "      <td>30</td>\n",
       "      <td>validation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  train_time  forecast_time        MAE  \\\n",
       "0             0         0.0    3.812914       4.442261  18.260277   \n",
       "1             1         0.0    3.485261       4.376619  28.395468   \n",
       "2             2         0.0    3.812914       4.442261  12.825266   \n",
       "3             3         0.0    3.485261       4.376619  26.284388   \n",
       "4             4         0.0    3.485261       4.376619  15.145726   \n",
       "\n",
       "           MSE       RMSE      MASE     RMSSE       MAPE      SMAPE  \\\n",
       "0   619.323792  24.886217  0.896638  0.792087        inf  16.386864   \n",
       "1  1160.189331  34.061552  1.088141  0.918072  41.157335  17.157184   \n",
       "2   269.356567  16.412086  0.814535  0.757765  39.022604  13.038494   \n",
       "3  1121.348877  33.486548  0.918232  0.792925  57.078475  18.962322   \n",
       "4   429.357697  20.720948  0.663444  0.597604  21.996029  10.027625   \n",
       "\n",
       "           naming_orig                                   model_name  \\\n",
       "0   danish_atm_daily_5  CatBoostAutoRegressivePipelineEtna_3lags_gl   \n",
       "1  danish_atm_daily_84  CatBoostAutoRegressivePipelineEtna_3lags_gl   \n",
       "2  danish_atm_daily_32  CatBoostAutoRegressivePipelineEtna_3lags_gl   \n",
       "3  danish_atm_daily_25  CatBoostAutoRegressivePipelineEtna_3lags_gl   \n",
       "4   danish_atm_daily_6  CatBoostAutoRegressivePipelineEtna_3lags_gl   \n",
       "\n",
       "       dataset_name  horizon       split  pred_time  \n",
       "0  danish_atm_daily       30        test        NaN  \n",
       "1  danish_atm_daily       30  validation        NaN  \n",
       "2  danish_atm_daily       30        test        NaN  \n",
       "3  danish_atm_daily       30  validation        NaN  \n",
       "4  danish_atm_daily       30  validation        NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "naming_orig\n",
       "mipt_alpha_193    44\n",
       "mipt_alpha_342    44\n",
       "mipt_alpha_510    44\n",
       "mipt_alpha_151    44\n",
       "mipt_alpha_584    44\n",
       "                  ..\n",
       "nn5_23            34\n",
       "nn5_2             34\n",
       "nn5_50            34\n",
       "nn5_111            6\n",
       "nn5_112            6\n",
       "Name: count, Length: 876, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['naming_orig'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the time-series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = Path.cwd() / 'time_series'\n",
    "folders = list(input_dir.glob(\"*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544221"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcualtion of the initial time-series value counts\n",
    "\n",
    "ts_full = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        time_series = pd.read_csv(path)\n",
    "        time_series['naming_orig'] = path.stem\n",
    "        ts_full = pd.concat([ts_full, time_series])\n",
    "\n",
    "ts_full.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All time-series has 544221 values initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nn5_112', 'nn5_111']\n"
     ]
    }
   ],
   "source": [
    "li1 = list(df['naming_orig'].unique())\n",
    "li2 = list(ts_full['naming_orig'].unique())\n",
    "temp3 = []\n",
    "for element in li1:\n",
    "    if element not in li2:\n",
    "        temp3.append(element)\n",
    " \n",
    "print(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also `nn5_112`, `nn5_111` time-series dropped, because they presented only in dataset with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491866"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping of last 60 days\n",
    "\n",
    "ts = pd.DataFrame()\n",
    "\n",
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        time_series = pd.read_csv(path)\n",
    "        if time_series.shape[0] > 60:\n",
    "            time_series = time_series[:-60]\n",
    "            time_series['naming_orig'] = path.stem\n",
    "            ts = pd.concat([ts, time_series])\n",
    "\n",
    "ts.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After droping last 60 days all time-series has 491866 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "danish_atm_daily_110 leghts: 42\n",
      "danish_atm_daily_111 leghts: 28\n",
      "danish_atm_daily_112 leghts: 25\n"
     ]
    }
   ],
   "source": [
    "# Finding such time-series that has less than 60 days\n",
    "\n",
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        time_series = pd.read_csv(path)\n",
    "        if time_series.shape[0] < 60:\n",
    "            print(f'{path.stem} leghts: {time_series.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 3 dropped from the consideration, because it has less than 60 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of all feature extraction methods together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series_features(dataset):\n",
    "    # Reformat 'date' column to datetime and setting it as index\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "    dataset.set_index('date', inplace=True)\n",
    "    \n",
    "    # Using of 'value' column as time-series\n",
    "    time_series = dataset['value']\n",
    "    \n",
    "    # Initialization of fetures' dictionary\n",
    "    features = {}\n",
    "    \n",
    "    # Time fetures of time series\n",
    "    features['trend_slope'], _, _, _, _ = linregress(np.arange(len(time_series)), time_series)\n",
    "    acf_values = acf(time_series, nlags=24, fft=True)\n",
    "    features['seasonality_strength'] = np.max(acf_values[1:])\n",
    "    features['num_peaks'] = len(find_peaks(time_series)[0])\n",
    "    features['diff_mean'] = time_series.diff().mean()\n",
    "    features['diff_std'] = time_series.diff().std()\n",
    "    zcr = ((time_series[:-1] * time_series[1:]) < 0).sum()\n",
    "    features['zero_crossing_rate'] = zcr\n",
    "    \n",
    "    # Spectral fetures\n",
    "    fft_vals = fft(time_series.to_numpy())\n",
    "    fft_freq = fftfreq(len(fft_vals), d=1)\n",
    "    dominant_freq = fft_freq[np.argmax(np.abs(fft_vals)**2)]\n",
    "    features['dominant_freq'] = np.abs(dominant_freq)\n",
    "    significant_freqs = np.sum(np.abs(fft_vals)**2 > 1e-5)\n",
    "    features['significant_freqs'] = significant_freqs\n",
    "    for freq in range(1, 6):\n",
    "        freq_mask = (fft_freq >= freq - 0.5) & (fft_freq <= freq + 0.5)\n",
    "        features[f'fft_energy_freq_{freq}'] = np.sum(np.abs(fft_vals[freq_mask])**2)\n",
    "    \n",
    "    # Morphological fetures\n",
    "    peaks, _ = find_peaks(time_series)\n",
    "    peak_distances = np.diff(peaks)\n",
    "    features['mean_peak_distance'] = np.mean(peak_distances) if len(peak_distances) > 0 else 0\n",
    "    features['std_peak_distance'] = np.std(peak_distances) if len(peak_distances) > 0 else 0\n",
    "    peak_heights = time_series.iloc[peaks]\n",
    "    features['mean_peak_height'] = peak_heights.mean()\n",
    "    features['std_peak_height'] = peak_heights.std()\n",
    "    \n",
    "    # Additional features\n",
    "    extrema = np.diff(np.sort(np.concatenate([find_peaks(time_series)[0], find_peaks(-time_series)[0]])))\n",
    "    features['mean_extrema_diff'] = np.mean(extrema) if len(extrema) > 0 else 0\n",
    "    features['std_extrema_diff'] = np.std(extrema) if len(extrema) > 0 else 0\n",
    "    curvature = np.diff(time_series, n=2).mean()\n",
    "    features['mean_curvature'] = curvature\n",
    "    magnitude = np.abs(fft_vals)\n",
    "    normalized_magnitude = magnitude / np.sum(magnitude)\n",
    "    signal_entropy = -np.sum(normalized_magnitude * np.log(normalized_magnitude + np.finfo(float).eps))\n",
    "    features['signal_entropy'] = signal_entropy\n",
    "    total_energy = np.sum(np.square(time_series))\n",
    "    features['total_energy'] = total_energy\n",
    "    features['diff_skew'] = skew(time_series.diff().dropna())\n",
    "    features['diff_kurt'] = kurtosis(time_series.diff().dropna())\n",
    "    features['time_series_length'] = len(time_series)\n",
    "    features['mae_diff'] = np.mean(np.abs(time_series.diff().dropna()))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 80/80 [01:24<00:00,  1.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naming_orig</th>\n",
       "      <th>value__variance_larger_than_standard_deviation</th>\n",
       "      <th>value__has_duplicate_max</th>\n",
       "      <th>value__has_duplicate_min</th>\n",
       "      <th>value__has_duplicate</th>\n",
       "      <th>value__sum_values</th>\n",
       "      <th>value__abs_energy</th>\n",
       "      <th>value__mean_abs_change</th>\n",
       "      <th>value__mean_change</th>\n",
       "      <th>value__mean_second_derivative_central</th>\n",
       "      <th>...</th>\n",
       "      <th>std_peak_height</th>\n",
       "      <th>mean_extrema_diff</th>\n",
       "      <th>std_extrema_diff</th>\n",
       "      <th>mean_curvature</th>\n",
       "      <th>signal_entropy</th>\n",
       "      <th>total_energy</th>\n",
       "      <th>diff_skew</th>\n",
       "      <th>diff_kurt</th>\n",
       "      <th>time_series_length</th>\n",
       "      <th>mae_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>danish_atm_daily_0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35327.000000</td>\n",
       "      <td>4.805377e+06</td>\n",
       "      <td>35.180921</td>\n",
       "      <td>0.398026</td>\n",
       "      <td>-0.287129</td>\n",
       "      <td>...</td>\n",
       "      <td>53.571178</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.807847</td>\n",
       "      <td>-0.574257</td>\n",
       "      <td>5.100711</td>\n",
       "      <td>4.805377e+06</td>\n",
       "      <td>0.141664</td>\n",
       "      <td>5.759565</td>\n",
       "      <td>305.0</td>\n",
       "      <td>35.180921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>danish_atm_daily_1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25074.000000</td>\n",
       "      <td>3.597174e+06</td>\n",
       "      <td>35.873096</td>\n",
       "      <td>-0.015228</td>\n",
       "      <td>0.443878</td>\n",
       "      <td>...</td>\n",
       "      <td>49.935416</td>\n",
       "      <td>1.710526</td>\n",
       "      <td>0.834809</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>4.574701</td>\n",
       "      <td>3.597174e+06</td>\n",
       "      <td>0.406797</td>\n",
       "      <td>3.227857</td>\n",
       "      <td>198.0</td>\n",
       "      <td>35.873096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>danish_atm_daily_10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27172.000000</td>\n",
       "      <td>3.278432e+06</td>\n",
       "      <td>31.470395</td>\n",
       "      <td>0.319079</td>\n",
       "      <td>-0.206271</td>\n",
       "      <td>...</td>\n",
       "      <td>70.440769</td>\n",
       "      <td>1.935897</td>\n",
       "      <td>1.217814</td>\n",
       "      <td>-0.412541</td>\n",
       "      <td>5.294722</td>\n",
       "      <td>3.278432e+06</td>\n",
       "      <td>1.406318</td>\n",
       "      <td>8.212164</td>\n",
       "      <td>305.0</td>\n",
       "      <td>31.470395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>danish_atm_daily_100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12234.000000</td>\n",
       "      <td>6.264840e+05</td>\n",
       "      <td>18.096346</td>\n",
       "      <td>-0.016611</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.974311</td>\n",
       "      <td>1.728324</td>\n",
       "      <td>0.826691</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>5.210385</td>\n",
       "      <td>6.264840e+05</td>\n",
       "      <td>-0.076666</td>\n",
       "      <td>2.179983</td>\n",
       "      <td>302.0</td>\n",
       "      <td>18.096346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>danish_atm_daily_101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13813.000000</td>\n",
       "      <td>1.375921e+06</td>\n",
       "      <td>34.407821</td>\n",
       "      <td>-0.217877</td>\n",
       "      <td>-0.081461</td>\n",
       "      <td>...</td>\n",
       "      <td>45.951714</td>\n",
       "      <td>1.712871</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>-0.162921</td>\n",
       "      <td>4.689043</td>\n",
       "      <td>1.375921e+06</td>\n",
       "      <td>0.136356</td>\n",
       "      <td>2.367263</td>\n",
       "      <td>180.0</td>\n",
       "      <td>34.407821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>nn5_95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15562.835350</td>\n",
       "      <td>3.864449e+05</td>\n",
       "      <td>7.733223</td>\n",
       "      <td>0.012809</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>...</td>\n",
       "      <td>7.677542</td>\n",
       "      <td>1.766990</td>\n",
       "      <td>0.872491</td>\n",
       "      <td>0.012989</td>\n",
       "      <td>5.944508</td>\n",
       "      <td>3.864449e+05</td>\n",
       "      <td>-0.730954</td>\n",
       "      <td>1.531685</td>\n",
       "      <td>731.0</td>\n",
       "      <td>7.733223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>nn5_96</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9925.473435</td>\n",
       "      <td>1.695436e+05</td>\n",
       "      <td>5.359904</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>...</td>\n",
       "      <td>6.711614</td>\n",
       "      <td>1.647059</td>\n",
       "      <td>0.750477</td>\n",
       "      <td>0.009345</td>\n",
       "      <td>5.990598</td>\n",
       "      <td>1.695436e+05</td>\n",
       "      <td>0.087705</td>\n",
       "      <td>1.590454</td>\n",
       "      <td>731.0</td>\n",
       "      <td>5.359904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>nn5_97</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14966.333509</td>\n",
       "      <td>3.548706e+05</td>\n",
       "      <td>7.519889</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>-0.001813</td>\n",
       "      <td>...</td>\n",
       "      <td>8.155347</td>\n",
       "      <td>1.615213</td>\n",
       "      <td>1.025204</td>\n",
       "      <td>-0.003626</td>\n",
       "      <td>5.929239</td>\n",
       "      <td>3.548706e+05</td>\n",
       "      <td>-0.586895</td>\n",
       "      <td>1.892068</td>\n",
       "      <td>731.0</td>\n",
       "      <td>7.519889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>nn5_98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10153.037875</td>\n",
       "      <td>1.618989e+05</td>\n",
       "      <td>3.804162</td>\n",
       "      <td>0.010052</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>...</td>\n",
       "      <td>5.250554</td>\n",
       "      <td>2.275000</td>\n",
       "      <td>1.249750</td>\n",
       "      <td>0.008677</td>\n",
       "      <td>5.920899</td>\n",
       "      <td>1.618989e+05</td>\n",
       "      <td>-0.488383</td>\n",
       "      <td>2.809081</td>\n",
       "      <td>731.0</td>\n",
       "      <td>3.804162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>nn5_99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9854.813256</td>\n",
       "      <td>1.538853e+05</td>\n",
       "      <td>4.213410</td>\n",
       "      <td>0.007656</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>...</td>\n",
       "      <td>5.457561</td>\n",
       "      <td>1.870801</td>\n",
       "      <td>1.023673</td>\n",
       "      <td>0.007505</td>\n",
       "      <td>5.929310</td>\n",
       "      <td>1.538853e+05</td>\n",
       "      <td>-0.392898</td>\n",
       "      <td>2.429767</td>\n",
       "      <td>731.0</td>\n",
       "      <td>4.213410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>871 rows × 834 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              naming_orig  value__variance_larger_than_standard_deviation  \\\n",
       "0      danish_atm_daily_0                                             1.0   \n",
       "1      danish_atm_daily_1                                             1.0   \n",
       "2     danish_atm_daily_10                                             1.0   \n",
       "3    danish_atm_daily_100                                             1.0   \n",
       "4    danish_atm_daily_101                                             1.0   \n",
       "..                    ...                                             ...   \n",
       "866                nn5_95                                             1.0   \n",
       "867                nn5_96                                             1.0   \n",
       "868                nn5_97                                             1.0   \n",
       "869                nn5_98                                             1.0   \n",
       "870                nn5_99                                             1.0   \n",
       "\n",
       "     value__has_duplicate_max  value__has_duplicate_min  value__has_duplicate  \\\n",
       "0                         0.0                       1.0                   1.0   \n",
       "1                         0.0                       1.0                   1.0   \n",
       "2                         0.0                       1.0                   1.0   \n",
       "3                         0.0                       1.0                   1.0   \n",
       "4                         0.0                       1.0                   1.0   \n",
       "..                        ...                       ...                   ...   \n",
       "866                       0.0                       1.0                   1.0   \n",
       "867                       0.0                       1.0                   1.0   \n",
       "868                       0.0                       1.0                   1.0   \n",
       "869                       0.0                       1.0                   1.0   \n",
       "870                       0.0                       1.0                   1.0   \n",
       "\n",
       "     value__sum_values  value__abs_energy  value__mean_abs_change  \\\n",
       "0         35327.000000       4.805377e+06               35.180921   \n",
       "1         25074.000000       3.597174e+06               35.873096   \n",
       "2         27172.000000       3.278432e+06               31.470395   \n",
       "3         12234.000000       6.264840e+05               18.096346   \n",
       "4         13813.000000       1.375921e+06               34.407821   \n",
       "..                 ...                ...                     ...   \n",
       "866       15562.835350       3.864449e+05                7.733223   \n",
       "867        9925.473435       1.695436e+05                5.359904   \n",
       "868       14966.333509       3.548706e+05                7.519889   \n",
       "869       10153.037875       1.618989e+05                3.804162   \n",
       "870        9854.813256       1.538853e+05                4.213410   \n",
       "\n",
       "     value__mean_change  value__mean_second_derivative_central  ...  \\\n",
       "0              0.398026                              -0.287129  ...   \n",
       "1             -0.015228                               0.443878  ...   \n",
       "2              0.319079                              -0.206271  ...   \n",
       "3             -0.016611                              -0.010000  ...   \n",
       "4             -0.217877                              -0.081461  ...   \n",
       "..                  ...                                    ...  ...   \n",
       "866            0.012809                               0.006494  ...   \n",
       "867            0.002342                               0.004672  ...   \n",
       "868            0.005747                              -0.001813  ...   \n",
       "869            0.010052                               0.004339  ...   \n",
       "870            0.007656                               0.003752  ...   \n",
       "\n",
       "     std_peak_height  mean_extrema_diff  std_extrema_diff  mean_curvature  \\\n",
       "0          53.571178           1.750000          0.807847       -0.574257   \n",
       "1          49.935416           1.710526          0.834809        0.887755   \n",
       "2          70.440769           1.935897          1.217814       -0.412541   \n",
       "3          21.974311           1.728324          0.826691       -0.020000   \n",
       "4          45.951714           1.712871          0.871287       -0.162921   \n",
       "..               ...                ...               ...             ...   \n",
       "866         7.677542           1.766990          0.872491        0.012989   \n",
       "867         6.711614           1.647059          0.750477        0.009345   \n",
       "868         8.155347           1.615213          1.025204       -0.003626   \n",
       "869         5.250554           2.275000          1.249750        0.008677   \n",
       "870         5.457561           1.870801          1.023673        0.007505   \n",
       "\n",
       "     signal_entropy  total_energy  diff_skew  diff_kurt  time_series_length  \\\n",
       "0          5.100711  4.805377e+06   0.141664   5.759565               305.0   \n",
       "1          4.574701  3.597174e+06   0.406797   3.227857               198.0   \n",
       "2          5.294722  3.278432e+06   1.406318   8.212164               305.0   \n",
       "3          5.210385  6.264840e+05  -0.076666   2.179983               302.0   \n",
       "4          4.689043  1.375921e+06   0.136356   2.367263               180.0   \n",
       "..              ...           ...        ...        ...                 ...   \n",
       "866        5.944508  3.864449e+05  -0.730954   1.531685               731.0   \n",
       "867        5.990598  1.695436e+05   0.087705   1.590454               731.0   \n",
       "868        5.929239  3.548706e+05  -0.586895   1.892068               731.0   \n",
       "869        5.920899  1.618989e+05  -0.488383   2.809081               731.0   \n",
       "870        5.929310  1.538853e+05  -0.392898   2.429767               731.0   \n",
       "\n",
       "      mae_diff  \n",
       "0    35.180921  \n",
       "1    35.873096  \n",
       "2    31.470395  \n",
       "3    18.096346  \n",
       "4    34.407821  \n",
       "..         ...  \n",
       "866   7.733223  \n",
       "867   5.359904  \n",
       "868   7.519889  \n",
       "869   3.804162  \n",
       "870   4.213410  \n",
       "\n",
       "[871 rows x 834 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = pd.DataFrame()\n",
    "features = pd.DataFrame()\n",
    "\n",
    "tsData = pd.read_csv('time_series\\mipt_alpha\\mipt_alpha_0.csv', index_col=0)\n",
    "pycatch22.CO_f1ecac(list(tsData['value']))\n",
    "\n",
    "\n",
    "# Initialization of the Catch22 and our own featurs dataframes\n",
    "catch22_df = pd.DataFrame(columns=pycatch22.catch22_all(list(tsData['value']), catch24=True)['names'])\n",
    "ord_features_df = pd.DataFrame(columns=list(extract_time_series_features(tsData).keys()))\n",
    "\n",
    "# Calculation of Catch22 and our own features across all time-series (last 60 days did not used)\n",
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        time_series = pd.read_csv(path)\n",
    "        if time_series.shape[0] > 60:\n",
    "            time_series = time_series[:-60]\n",
    "            time_series['naming_orig'] = path.stem\n",
    "            ts = pd.concat([ts, time_series])\n",
    "            pycatch22.CO_f1ecac(list(time_series['value']))\n",
    "            catch22_df.loc[len(catch22_df.index)] = pycatch22.catch22_all(list(time_series['value']),catch24=True)['values']\n",
    "            ord_features_df.loc[len(ord_features_df.index)] = list(extract_time_series_features(time_series).values())\n",
    "\n",
    "# Calculation of tsfresh features\n",
    "ts_tsf = tsf.extract_features(ts, column_id='naming_orig', \n",
    "                              column_sort=\"date\", column_value='value').rename_axis('naming_orig').reset_index()\n",
    "\n",
    "# Combinig of all datasets to one\n",
    "features = pd.concat([ts_tsf, catch22_df, ord_features_df], axis=1)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.to_csv('All_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tsfresh feature extraction separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 80/80 [01:33<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "ts_tsf = tsf.extract_features(ts, column_id='naming_orig', column_sort=\"date\", column_value='value').rename_axis('naming_orig').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['naming_orig', 'value__variance_larger_than_standard_deviation',\n",
       "       'value__has_duplicate_max', 'value__has_duplicate_min',\n",
       "       'value__has_duplicate', 'value__sum_values', 'value__abs_energy',\n",
       "       'value__mean_abs_change', 'value__mean_change',\n",
       "       'value__mean_second_derivative_central',\n",
       "       ...\n",
       "       'value__fourier_entropy__bins_5', 'value__fourier_entropy__bins_10',\n",
       "       'value__fourier_entropy__bins_100',\n",
       "       'value__permutation_entropy__dimension_3__tau_1',\n",
       "       'value__permutation_entropy__dimension_4__tau_1',\n",
       "       'value__permutation_entropy__dimension_5__tau_1',\n",
       "       'value__permutation_entropy__dimension_6__tau_1',\n",
       "       'value__permutation_entropy__dimension_7__tau_1',\n",
       "       'value__query_similarity_count__query_None__threshold_0.0',\n",
       "       'value__mean_n_absolute_max__number_of_maxima_7'],\n",
       "      dtype='object', length=784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_tsf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_tsf.to_csv('Tsfresh_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catch22 feature extraction separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsData = list(pd.read_csv('time_series\\mipt_alpha\\mipt_alpha_0.csv')['value'])\n",
    "pycatch22.CO_f1ecac(tsData)\n",
    "\n",
    "catch22_features = pycatch22.catch22_all(tsData,catch24=True)['names']\n",
    "catch22_features.append('naming_orig')\n",
    "catch22_df = pd.DataFrame(columns=catch22_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        tsData = list(pd.read_csv(path)['value'])\n",
    "        if len(tsData) > 60:\n",
    "            tsData = tsData[:-60]\n",
    "            pycatch22.CO_f1ecac(tsData)\n",
    "            values = pycatch22.catch22_all(tsData,catch24=True)['values']\n",
    "            values.append(path.stem)\n",
    "            catch22_df.loc[len(catch22_df.index)] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DN_HistogramMode_5', 'DN_HistogramMode_10', 'CO_f1ecac',\n",
       "       'CO_FirstMin_ac', 'CO_HistogramAMI_even_2_5', 'CO_trev_1_num',\n",
       "       'MD_hrv_classic_pnn40', 'SB_BinaryStats_mean_longstretch1',\n",
       "       'SB_TransitionMatrix_3ac_sumdiagcov', 'PD_PeriodicityWang_th0_01',\n",
       "       'CO_Embed2_Dist_tau_d_expfit_meandiff',\n",
       "       'IN_AutoMutualInfoStats_40_gaussian_fmmi',\n",
       "       'FC_LocalSimple_mean1_tauresrat', 'DN_OutlierInclude_p_001_mdrmd',\n",
       "       'DN_OutlierInclude_n_001_mdrmd', 'SP_Summaries_welch_rect_area_5_1',\n",
       "       'SB_BinaryStats_diff_longstretch0', 'SB_MotifThree_quantile_hh',\n",
       "       'SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1',\n",
       "       'SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1',\n",
       "       'SP_Summaries_welch_rect_centroid', 'FC_LocalSimple_mean3_stderr',\n",
       "       'DN_Mean', 'DN_Spread_Std', 'naming_orig'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch22_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch22_df.to_csv('Catch22_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary features separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series_features(dataset):\n",
    "    # Преобразование столбца 'date' в формат datetime и установка его как индекс\n",
    "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
    "    dataset.set_index('date', inplace=True)\n",
    "    \n",
    "    # Использование столбца 'value' как временного ряда\n",
    "    time_series = dataset['value']\n",
    "    \n",
    "    # Инициализация словаря для хранения фичей\n",
    "    features = {}\n",
    "    \n",
    "    # Временные признаки\n",
    "    features['trend_slope'], _, _, _, _ = linregress(np.arange(len(time_series)), time_series)\n",
    "    acf_values = acf(time_series, nlags=24, fft=True)\n",
    "    features['seasonality_strength'] = np.max(acf_values[1:])\n",
    "    features['num_peaks'] = len(find_peaks(time_series)[0])\n",
    "    features['diff_mean'] = time_series.diff().mean()\n",
    "    features['diff_std'] = time_series.diff().std()\n",
    "    zcr = ((time_series[:-1] * time_series[1:]) < 0).sum()\n",
    "    features['zero_crossing_rate'] = zcr\n",
    "    \n",
    "    # Спектральные признаки\n",
    "    fft_vals = fft(time_series.to_numpy())\n",
    "    fft_freq = fftfreq(len(fft_vals), d=1)\n",
    "    dominant_freq = fft_freq[np.argmax(np.abs(fft_vals)**2)]\n",
    "    features['dominant_freq'] = np.abs(dominant_freq)\n",
    "    significant_freqs = np.sum(np.abs(fft_vals)**2 > 1e-5)\n",
    "    features['significant_freqs'] = significant_freqs\n",
    "    for freq in range(1, 6):\n",
    "        freq_mask = (fft_freq >= freq - 0.5) & (fft_freq <= freq + 0.5)\n",
    "        features[f'fft_energy_freq_{freq}'] = np.sum(np.abs(fft_vals[freq_mask])**2)\n",
    "    \n",
    "    # Морфологические признаки\n",
    "    peaks, _ = find_peaks(time_series)\n",
    "    peak_distances = np.diff(peaks)\n",
    "    features['mean_peak_distance'] = np.mean(peak_distances) if len(peak_distances) > 0 else 0\n",
    "    features['std_peak_distance'] = np.std(peak_distances) if len(peak_distances) > 0 else 0\n",
    "    peak_heights = time_series.iloc[peaks]\n",
    "    features['mean_peak_height'] = peak_heights.mean()\n",
    "    features['std_peak_height'] = peak_heights.std()\n",
    "    \n",
    "    # Дополнительные фичи\n",
    "    extrema = np.diff(np.sort(np.concatenate([find_peaks(time_series)[0], find_peaks(-time_series)[0]])))\n",
    "    features['mean_extrema_diff'] = np.mean(extrema) if len(extrema) > 0 else 0\n",
    "    features['std_extrema_diff'] = np.std(extrema) if len(extrema) > 0 else 0\n",
    "    curvature = np.diff(time_series, n=2).mean()\n",
    "    features['mean_curvature'] = curvature\n",
    "    magnitude = np.abs(fft_vals)\n",
    "    normalized_magnitude = magnitude / np.sum(magnitude)\n",
    "    signal_entropy = -np.sum(normalized_magnitude * np.log(normalized_magnitude + np.finfo(float).eps))\n",
    "    features['signal_entropy'] = signal_entropy\n",
    "    total_energy = np.sum(np.square(time_series))\n",
    "    features['total_energy'] = total_energy\n",
    "    features['diff_skew'] = skew(time_series.diff().dropna())\n",
    "    features['diff_kurt'] = kurtosis(time_series.diff().dropna())\n",
    "    features['time_series_length'] = len(time_series)\n",
    "    features['mae_diff'] = np.mean(np.abs(time_series.diff().dropna()))\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsData = pd.read_csv('time_series\\mipt_alpha\\mipt_alpha_0.csv', index_col=0)\n",
    "\n",
    "features = list(extract_time_series_features(tsData).keys())\n",
    "features.append('naming_orig')\n",
    "ord_features_df = pd.DataFrame(columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsData = pd.read_csv('time_series\\mipt_alpha\\mipt_alpha_0.csv', index_col=0)\n",
    "\n",
    "features = list(extract_time_series_features(tsData).keys())\n",
    "features.append('naming_orig')\n",
    "ord_features_df = pd.DataFrame(columns=features)\n",
    "ord_features_df\n",
    "\n",
    "for folder in folders:\n",
    "    paths = list(folder.glob(\"*\"))\n",
    "    for path in paths:\n",
    "        tsData = pd.read_csv(path, index_col=0)\n",
    "        if tsData.shape[0] > 60:\n",
    "            values = list(extract_time_series_features(tsData).values())\n",
    "            values.append(path.stem)\n",
    "            ord_features_df.loc[len(ord_features_df.index)] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['trend_slope', 'seasonality_strength', 'num_peaks', 'diff_mean',\n",
       "       'diff_std', 'zero_crossing_rate', 'dominant_freq', 'significant_freqs',\n",
       "       'fft_energy_freq_1', 'fft_energy_freq_2', 'fft_energy_freq_3',\n",
       "       'fft_energy_freq_4', 'fft_energy_freq_5', 'mean_peak_distance',\n",
       "       'std_peak_distance', 'mean_peak_height', 'std_peak_height',\n",
       "       'mean_extrema_diff', 'std_extrema_diff', 'mean_curvature',\n",
       "       'signal_entropy', 'total_energy', 'diff_skew', 'diff_kurt',\n",
       "       'time_series_length', 'mae_diff', 'naming_orig'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord_features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_features_df.to_csv('Ordinary_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
